defaults:
  - callbacks: savi
  - hydra: default
  - logger: tensorboard
  - trainer: gradient_clipping
  - _self_
  # Local configuration for machine- or user-specific settings.
  - optional local: default.yaml

checkpoint: ""
experiment: "my_experiment"
seed: 42

dataset:
  _target_: datasets.image_folder.ImageFolderDataset
  path: /home/nfs/inf6/data/datasets/EwertzData/sold
  data_dir: pick_red
  num_workers: 8
  batch_size: 64

model:
  _target_: train_savi.SAViModule

  savi:
    _target_: modeling.savi.model.SAVi

    corrector:
      _target_: modeling.savi.corrector.Corrector
      num_slots: 10  # Number of slots to decompose the image.
      slot_dim: 64  # Dimension of each slot.
      feature_dim: ${..encoder.feature_dim}
      hidden_dim: 128
      num_iterations: 1
      num_initial_iterations: 3

    predictor:
      _target_: modeling.savi.predictor.TransformerPredictor
      slot_dim: ${..corrector.slot_dim}
      action_dim: '???'  # Dimension of the action-space gets queried from the dataset.

    encoder:
      _target_: modeling.savi.encoder.FullyConvolutionalEncoder
      image_size: '???'  # Image-size gets queried from the dataset.
      num_channels: [ 32, 32, 32, 32 ]
      kernel_size: 5
      feature_dim: 64

    decoder:
      _target_: modeling.savi.decoder.FullyConvolutionalDecoder
      image_size: '???'
      slot_dim: ${..corrector.slot_dim}
      in_channels: ${..corrector.slot_dim}
      num_channels: [ 32, 32, 32, 32 ]
      kernel_size: 5
      # upsample: 2  # To create upsampling decoder.

    initializer:
      _target_: modeling.savi.initializer.Learned
      num_slots: ${..corrector.num_slots}
      slot_dim: ${..corrector.slot_dim}

  optimizer:
    _target_: torch.optim.Adam
    _partial_: True
    lr: 0.0001

  scheduler:
    scheduler:
      _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
      _partial_: True
      warmup_epochs: 10  # Number of warmup epochs.
      max_epochs: ${....trainer.max_epochs}
    extras:
      interval: "epoch"
