experiment: default
seed: 42

dataset:
  _target_: sold.datasets.image_folder.ImageFolderDataset
  path: /home/user/mosbach/PycharmProjects/sold/data
  data_dir: test_dataset_0
  num_workers: 8
  batch_size: 64

trainer:
  _target_: lightning.Trainer
  max_steps: 31300  # 100 epochs
  gradient_clip_val: 0.05  # Calls torch.nn.utils.clip_grad_norm_() over all model parameters.

callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: 'step'

  checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    every_n_epochs: 3
    save_top_k: -1  # Save every checkpoint.
    filename: 'SAVi-{epoch:02d}-{val_loss:.6f}'

model:
  _target_: sold.savi.model.SAVi

  # Image and slot dimensions.
  image_size: [ 64, 64 ]
  num_slots: 6
  slot_dim: 64

  # Optimization parameters.
  learning_rate: 0.0001
  warmup_steps: 2000
  max_steps: ${..trainer.max_steps}

  # SAVi components.
  encoder:
    _target_: sold.savi.encoder.FullyConvolutionalEncoder
    image_size: ${..image_size}
    num_channels: [ 32, 32, 32, 32 ]
    kernel_size: 5
    feature_dim: 64

  decoder:
    _target_: sold.savi.decoder.FullyConvolutionalDecoder
    in_channels: ${..slot_dim}
    num_channels: [ 32, 32, 32, 32 ]
    kernel_size: 5

  initializer:
    _target_: sold.savi.initializer.Learned
    num_slots: ${..num_slots}
    slot_dim: ${..slot_dim}

  predictor:
    _target_: sold.savi.predictor.TransformerPredictor
    slot_dim: ${..slot_dim}

  corrector:
    _target_: sold.savi.corrector.Corrector
    num_slots: ${..num_slots}
    slot_dim: ${..slot_dim}
    feature_dim: ${..encoder.feature_dim}
    hidden_dim: 128
    num_iterations: 1
    num_initial_iterations: 3

hydra:
  run:
    dir: experiments/savi/${...experiment}/${now:%Y-%m-%d_%H-%M-%S}
