callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    every_n_epochs: 1
    save_top_k: -1
    filename: ${hydra:job.config_name}-{epoch:02d}-{val_loss:.6f}
  savi_decomposition:
    _target_: sold.utils.visualization.SAViDecomposition
    every_n_epochs: 1
    save_dir: images
experiment:
  name: reach_red_3
  seed: 3
logger:
  tensorboard:
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    save_dir: ''
    log_graph: false
trainer:
  _target_: lightning.Trainer
  max_steps: 1000000
  gradient_clip_val: 0.05
dataset:
  _target_: sold.datasets.image_folder.ImageFolderDataset
  path: /home/nfs/inf6/data/datasets/EwertzData/sold
  data_dir: reach_red_new
  num_workers: 8
  batch_size: 64
model:
  _target_: sold.training.train_savi.SAViTrainer
  savi:
    _target_: sold.models.savi.model.SAVi
    corrector:
      _target_: sold.models.savi.corrector.Corrector
      num_slots: 7
      slot_dim: 64
      feature_dim: ${..encoder.feature_dim}
      hidden_dim: 128
      num_iterations: 1
      num_initial_iterations: 3
    predictor:
      _target_: sold.models.savi.predictor.TransformerPredictor
      slot_dim: ${..corrector.slot_dim}
    encoder:
      _target_: sold.models.savi.encoder.FullyConvolutionalEncoder
      image_size:
      - 64
      - 64
      num_channels:
      - 32
      - 32
      - 32
      - 32
      kernel_size: 5
      feature_dim: 64
    decoder:
      _target_: sold.models.savi.decoder.FullyConvolutionalDecoder
      image_size: ${..encoder.image_size}
      slot_dim: ${..corrector.slot_dim}
      in_channels: ${..corrector.slot_dim}
      num_channels:
      - 32
      - 32
      - 32
      - 32
      kernel_size: 5
    initializer:
      _target_: sold.models.savi.initializer.Learned
      num_slots: ${..corrector.num_slots}
      slot_dim: ${..corrector.slot_dim}
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.0001
  scheduler:
    _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
    _partial_: true
    warmup_epochs: 2000
    max_epochs: ${...trainer.max_steps}
