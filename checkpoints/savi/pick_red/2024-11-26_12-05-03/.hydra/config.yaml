callbacks:
  learning_rate:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    every_n_epochs: 100
    save_top_k: -1
    filename: ${hydra:job.config_name}-{epoch:02d}-{valid_loss:.6f}
  savi_decomposition:
    _target_: sold.utils.logging.LogDecomposition
    every_n_epochs: 100
    save_dir: images
experiment:
  name: pick_red
  seed: 42
logger:
  tensorboard:
    _target_: sold.utils.logging.ExtendedTensorBoardLogger
    name: logs
    save_dir: ''
    log_graph: false
  log_to_wandb: false
trainer:
  _target_: lightning.Trainer
  max_steps: 500000
  gradient_clip_val: 0.05
dataset:
  _target_: sold.datasets.image_folder.ImageFolderDataset
  path: /home/nfs/inf6/data/datasets/EwertzData/sold
  data_dir: pick_red
  num_workers: 8
  batch_size: 64
model:
  _target_: sold.training.train_savi.SAViTrainer
  savi:
    _target_: sold.modeling.savi.model.SAVi
    corrector:
      _target_: sold.modeling.savi.corrector.Corrector
      num_slots: 10
      slot_dim: 64
      feature_dim: ${..encoder.feature_dim}
      hidden_dim: 128
      num_iterations: 1
      num_initial_iterations: 3
    predictor:
      _target_: sold.modeling.savi.predictor.TransformerPredictor
      slot_dim: ${..corrector.slot_dim}
    encoder:
      _target_: sold.modeling.savi.encoder.FullyConvolutionalEncoder
      image_size:
      - 64
      - 64
      num_channels:
      - 32
      - 32
      - 32
      - 32
      kernel_size: 5
      feature_dim: 64
    decoder:
      _target_: sold.modeling.savi.decoder.FullyConvolutionalDecoder
      image_size: ${..encoder.image_size}
      slot_dim: ${..corrector.slot_dim}
      in_channels: ${..corrector.slot_dim}
      num_channels:
      - 32
      - 32
      - 32
      - 32
      kernel_size: 5
    initializer:
      _target_: sold.modeling.savi.initializer.Learned
      num_slots: ${..corrector.num_slots}
      slot_dim: ${..corrector.slot_dim}
  optimizer:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 0.0001
  scheduler:
    scheduler:
      _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
      _partial_: true
      warmup_epochs: 2000
      max_epochs: ${....trainer.max_steps}
    extras:
      interval: step
