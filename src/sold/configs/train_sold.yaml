defaults:
  - callbacks: sold
  - hydra: default
  - logger: tensorboard
  - trainer: online

checkpoint: "" #"/home/user/mosbach/PycharmProjects/sold/experiments/sold/reach_red2_baseline/2024-12-02_12-40-09/logs/version_0/checkpoints/sold-steps=42525-episode=1702-eval_episode_return=32.20086122869824.ckpt"
experiment: "my_experiment"
seed: 42

model:
  _target_: train_sold.SOLDModule

  max_steps: 1000000  # Maximum number of environment steps to train for.
  num_seed: 250  # Start training after 'train_after' random steps.
  update_freq: 25  # Update the models every 'update_freq' steps.
  num_updates: 10  # Number of updates to perform whenever the models are updated.
  eval_freq: 2500  # Evaluate the models every 'eval_freq' steps.
  num_eval_episodes: 10  # Number of episodes to collect during evaluation.
  batch_size: 8  # Batch size for training.
  buffer_capacity: 1250000  # Maximum number of time-steps to store in the replay buffer.

  dynamics_learning_rate: 0.0001
  dynamics_grad_clip: 3.0
  actor_learning_rate: 0.00003
  actor_grad_clip: 10.0
  actor_entropy_loss_weight: 0.0003
  actor_gradients: "dynamics"  # Use "dynamics" or "reinforce" gradients for the actor.
  critic_learning_rate: 0.00003
  critic_grad_clip: 10.0
  reward_learning_rate: 0.0001
  reward_grad_clip: 10.0

  finetune_savi: False  # Whether to finetune the SAVI model.
  savi_learning_rate: 0.0001
  savi_grad_clip: 0.05

  num_context: [3, 3]  # [min, max] number of context frames given to the model during dynamics learning and latent imagination.
  imagination_horizon: 15  # Number of frames to predict in imagination.
  start_imagination_from_every: False  # Whether to start imagination from possible frame in the sequences or only a single one.

  return_lambda: 0.95  # Lambda used to compute bootstrapped Î»-returns.
  discount_factor: 0.96  # Discount factor used to compute returns.
  critic_ema_decay: 0.98  # Exponential moving average decay for the critic target network.

  env:
    _target_: envs.make_env
    suite: "mof"  # Should be "mof", "gym", or "dmcontrol".
    name: ReachRed_0to4Distractors_Dense-v1
    image_size: [ 64, 64 ]
    max_episode_steps: 50
    action_repeat: 2

  savi:
    _target_: train_savi.load_savi
    checkpoint_path: ../../../../checkpoints/savi/reach_red/remapped.zip

  dynamics_predictor:
    _target_: modeling.sold.dynamics.make_ocvp_seq_dynamics_model
    _partial_: True
    token_dim: 256
    hidden_dim: 512
    num_layers: 4
    num_heads: 8
    residual: True
    teacher_forcing: False

  actor:
    _target_: modeling.sold.prediction.GaussianPredictor
    _partial_: True
    token_dim: 256
    num_heads: 8
    num_layers: 4
    hidden_dim: 512

  critic:
    _target_: modeling.sold.prediction.TwoHotPredictor
    _partial_: True
    token_dim: 256
    num_heads: 8
    num_layers: 4
    hidden_dim: 512

  reward_predictor:
    _target_: modeling.sold.prediction.TwoHotPredictor
    _partial_: True
    token_dim: 256
    num_heads: 8
    num_layers: 4
    hidden_dim: 512
